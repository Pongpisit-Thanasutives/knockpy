{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll review how to use ``knockpy`` to apply the knockoff framework for selective inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics of knockoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we briefly review the knockoff framework. Users already familiar with knockoffs may want to scroll past this section.\n",
    "\n",
    "Given a set of $p$ features $X = (X_1, \\dots, X_p)$ and an outcome of interest $y$, knockoffs aim to select the small fraction of features on which $y$ actually depends while controlling the false discovery rate. For example, if $y \\mid X \\sim \\mathcal{N}(X \\beta, \\sigma^2)$ and $\\beta$ is sparse, knockoffs aim to identify the set $\\{j : \\beta_j \\ne 0\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the knockoffs framework involves executing three steps.\n",
    "\n",
    "1. First, we construct synthetic variables $\\tilde{X} = (\\tilde{X}_1, \\dots, \\tilde{X}_p)$ called knockoffs. Intuitively, the $j$th knockoff $\\tilde{X}_j$ acts as a \"negative control\" on the $j$th feature $X_j$ during variable selection. In ``knockpy``, knockoffs are denoted as the numpy array ``Xk``. \n",
    "\n",
    "2. Second, we use an arbitrary machine learning algorithm -- usually called a *feature statistic* -- to assign variable importances to each of the $p$ features and each of the $p$ knockoffs. For example, we might train a cross-validated Lasso on $[X, \\tilde{X}]$ and $y$ and use the lasso coefficient sizes as a measure of variable importance.\n",
    "\n",
    "3. Intuitively, a non-null feature should be assigned a higher variable importance than its (synthetic) knockoff, whereas knockoffs are constructed such that null features are indistinguishable from their knockoff. The *data-dependent-threshhold* introduced in [Barber and Candes 2015](https://arxiv.org/abs/1404.5609) formalizes this intuition and uses the feature statistics to reject a set of variables such that the expected fraction of false positives is below a prespecified proportion $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of knockoffs:\n",
    "\n",
    "1. **Fixed-X** knockoffs treat the design matrix $X$ as fixed and control the false discovery rate assuming $y \\mid X$ follows a homoskedastic gaussian linear response. In this case, it is possible to construct valid knockoffs $\\tilde{X}$ with no assumptions on $X$. Note also that when using fixed-X knockoffs, the feature statistic must satisfy a slightly more restrictive *sufficiency* condition (see [Barber and Candes 2015](https://arxiv.org/abs/1404.5609)).\n",
    "\n",
    "2. **Model-X** knockoffs treat the design matrix $X$ as random. Model-X knockoffs control the false discovery rate for any conditional distribution $y \\mid X$, but they crucially assume that the distribution of $X$ is known. Thus, to construct model-X knockoffs, one must know (or at least estimate) the distribution of $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics of knockpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``knockpy.knockoff_filter.KnockoffFilter`` class in ``knockpy`` generates knockoffs, fits the feature statistics, and applies the data-dependent threshhold all at once. This is demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a synthetic dataset where $X \\sim \\mathcal{N}(0, \\Sigma)$ for some $\\Sigma$ and $y \\mid X$ Gaussian with homoskedastic errors. The details of this dataset are commented below, but they aren't too important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random covariance matrix for X\n",
    "import numpy as np\n",
    "import knockpy\n",
    "\n",
    "np.random.seed(110)\n",
    "n = 200 # number of data points\n",
    "p = 500  # number of features\n",
    "Sigma = knockpy.dgp.AR1(p=p, rho=0.5) # Stationary AR1 process with correlation 0.5\n",
    "\n",
    "# Sample X\n",
    "X = np.random.multivariate_normal(mean=np.zeros(p), cov=Sigma, size=(n,))\n",
    "\n",
    "# Create random sparse coefficients\n",
    "beta = knockpy.dgp.create_sparse_coefficients(p=p, sparsity=0.1)\n",
    "y = np.dot(X, beta) + np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the ``KnockoffFilter`` class. To do this, we need to specify (i) what type of knockoff sampler we will use and (ii) what feature statistic we are using. Since $X \\sim \\mathcal{N}(0, \\Sigma)$, we will use Gaussian knockoffs, and we'll use the Lasso as our feature statistic, since it's a good all-around choice. We'll explore more options for these arguments later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knockpy.knockoff_filter import KnockoffFilter\n",
    "kfilter = KnockoffFilter(\n",
    "    ksampler='gaussian',\n",
    "    fstat='lasso',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the knockoff filter on our data! Since we are using a model-X approach, we initially pass $\\Sigma$ as an input to the knockoff filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The knockoff filter has discovered 68.0% of the non-nulls with an FDR of 19.0476194024086%\n"
     ]
    }
   ],
   "source": [
    "# Flags of whether each feature was rejected\n",
    "rejections = kfilter.forward(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    Sigma=Sigma,\n",
    "    fdr=0.1 # desired level of false discovery rate control\n",
    ")\n",
    "# Check the number of discoveries we made\n",
    "power = np.dot(rejections, beta != 0) / (beta != 0).sum()\n",
    "fdr = np.dot(rejections, beta == 0) / rejections.sum()\n",
    "print(f\"The knockoff filter has discovered {100*power}% of the non-nulls with an FDR of {100*fdr}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in most real applications, we do not know $\\Sigma$. In these cases, the knockoff filter will automatically infer $\\Sigma$ using LedoitWolf or GraphicalLasso covariance estimation. Although this invalidates the exact validty of model-X knockoffs, knockoffs have been shown to be fairly robust in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The knockoff filter has discovered 64.0% of the non-nulls with an FDR of 3.030303120613098%\n"
     ]
    }
   ],
   "source": [
    "# Try again with estimated cov matrix\n",
    "kfilter2 = KnockoffFilter(ksampler='gaussian', fstat='lasso')\n",
    "rejections = kfilter.forward(X=X, y=y, fdr=0.1)\n",
    "# Check the number of discoveries we made\n",
    "power = np.dot(rejections, beta != 0) / (beta != 0).sum()\n",
    "fdr = np.dot(rejections, beta == 0) / rejections.sum()\n",
    "print(f\"The knockoff filter has discovered {100*power}% of the non-nulls with an FDR of {100*fdr}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knockoff sampling - Knockoffs galore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolized Knockoff Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``knockpy`` implements a fully general covariance-guided Metropolized Knockoff Sampler, which is capable of sampling model-X knockoffs for any $X$ distribution given an (unnormalized) density function of $X$. This metropolized knockoff sampler uses a variety of computational tricks to make it orders of magnitude faster than a naive implementation. \n",
    "\n",
    "We review metro in more detail in the advanced usage section. For now, we give an example of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Precision matrix Q is not compatible with undirected graph (nonedge has value 0.2518527691128594)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-718caaa5b45d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmetrosampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknockpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetropolizedKnockoffSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mundir_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mXk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrosampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_knockoffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/knockpy/metro.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lf, X, mu, Sigma, undir_graph, order, active_frontier, gamma, metro_verbose, cliques, log_potentials, buckets, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_nonedge\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 157\u001b[0;31m                         \u001b[0;34mf\"Precision matrix Q is not compatible with undirected graph (nonedge has value {max_nonedge})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                     )\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Precision matrix Q is not compatible with undirected graph (nonedge has value 0.2518527691128594)"
     ]
    }
   ],
   "source": [
    "import knockpy.metro\n",
    "\n",
    "# Fake variables for simplicity.\n",
    "p = 30\n",
    "X = np.random.randn(n, p)\n",
    "\n",
    "# An arbitrary log-likelihood function\n",
    "beta = np.random.randn(p)\n",
    "def log_likelihood(X):\n",
    "    X[0:-1]*beta[0:-1]*np.abs(X[1:0])\n",
    "    \n",
    "# Undirected graph \n",
    "U = np.zeros((p,p))\n",
    "for xcoord in range(p):\n",
    "    for offset in [-2, 1, 0, 1, 2]:\n",
    "        ycoord = min(max(0, xcoord + offset), p-1)\n",
    "        U[xcoord, ycoord] = 1\n",
    "        \n",
    "\n",
    "metrosampler = knockpy.metro.MetropolizedKnockoffSampler(log_likelihood, X=X, undir_graph=U)\n",
    "Xk = metrosampler.sample_knockoffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to build a custom knockoff sampler class, as long as it inherets from the base class ``knockpy.knockoffs.KnockoffSampler``, you can still pass it to the KnockoffFilter constructor to run the knockoff filter. For example, we pass the customized metropolized knockoff sampler to a KnockoffFilter below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrosampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-81475bd01dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkfilter_metro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKnockoffFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mksampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrosampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfstat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ridge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'metrosampler' is not defined"
     ]
    }
   ],
   "source": [
    "kfilter_metro = KnockoffFilter(ksampler=metrosampler, fstat='ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metro module can also accept clique potentials from an undirected graphical model in place of the likelihood function to get a $O(p)$ speedup, as discussed in advanced usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of knockoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to generate knockoffs, including \"MAC-minimizing\" or \"SDP\" knockoffs, \"MVR\" knockoffs, \"MMI\" knockoffs, and \"conditional independence\" (CI) knockoffs. ``knockpy`` supports all of these knockoff generation methods for \"gaussian\" and \"fixed-X\" knockoff types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolized sampler for heavy-tailed t markov chain using MVR-guided proposals\n",
    "kfilter1 = KnockoffFilter(ksampler='artk', knockoff_kwargs={'method':'mvr'})\n",
    "\n",
    "# This uses gaussian MMI knockoffs\n",
    "kfilter2 = KnockoffFilter(ksampler='gaussian', knockoff_kwargs={'method':'mmi'})\n",
    "\n",
    "# This uses fixed-X SDP knockoffs\n",
    "kfilter3 = KnockoffFilter(ksampler='fx', knockoff_kwargs={'method':'sdp'})\n",
    "\n",
    "# The 'method' options include: equicorrelated, sdp, mvr, mmi, and ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knockpy provides this functionality by offering very fast solvers for generating the knockoff $S$-matrix, as detailed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For p=500, MVR took 2.6 sec, MMI took 4.6 sec, SDP took 7.4 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time0 = time.time()\n",
    "S_MVR = knockpy.mrc.solve_mvr(Sigma)\n",
    "time1 = time.time()\n",
    "mvr_time = np.around(time1 - time0, 1)\n",
    "S_MMI = knockpy.mrc.solve_mmi(Sigma)\n",
    "time2 = time.time()\n",
    "mmi_time = np.around(time2 - time1, 1)\n",
    "S_SDP = knockpy.mac.solve_SDP(Sigma)\n",
    "time3 = time.time()\n",
    "sdp_time = np.around(time3 - time2, 1)\n",
    "print(f\"For p={Sigma.shape[0]}, MVR took {mvr_time} sec, MMI took {mmi_time} sec, SDP took {sdp_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``knockpy`` offers a whole suite of built-in feature statistics, including cross-validated lasso, ridge, and group-lasso coefficients, lasso-path statistics, the deepPINK statistic [(Lu et. al 2018)](https://arxiv.org/abs/1809.01185), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tidbits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
